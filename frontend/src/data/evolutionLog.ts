// Human Stewardship Log - Documenting the human-AI collaboration
// Each entry shows: Problem Identified → Action Taken → Result

export interface StewardshipEntry {
  id: string;
  date: string;
  title: string;
  problem: string;
  action: string;
  result: string;
  category: 'learning' | 'optimization' | 'capability' | 'foundation';
  commitHash?: string;
}

export const stewardshipLog: StewardshipEntry[] = [
  {
    id: 'pipeline-false-negatives',
    date: '2026-02-12',
    title: 'Fix Pipeline False Negatives & Scraper Noise',
    problem: 'For 10+ days, agent reported "2 accepted" daily but no new events reached the calendar. Root cause: checkAustinLocation() was returning false (reject) instead of null (uncertain) for events with location data that didn\'t match hardcoded Austin indicators — blocking legitimate Austin events from trusted sources without Claude verification. Additionally, AI Accelerator scraper was returning ~21 non-Austin events per day (Boston, NY, London) that passed through to the pipeline, wasting processing. Three sources (HackAI, Austin Forum, Leaders in AI) silently returning 0 events.',
    action: 'Changed checkAustinLocation() to return null (needs verification) instead of false for ambiguous locations — only returns false when a non-Austin city is positively identified. Added title to the location check (catches "ATX", "SXSW" in event names). Expanded Austin indicators with 15+ new entries (atx, sxsw, south lamar, mueller, zilker, congress ave, etc.). Fixed AI Accelerator scraper: added Austin filtering at scraper level, removed false "Austin" default for venue_name, fixed source_event_id extraction for trailing-slash URLs. Added diagnostic logging to all 3 dead sources.',
    result: 'Ambiguous events now go to Claude for verification instead of being silently rejected. AI Accelerator scraper filters non-Austin events before they enter the pipeline. Dead source diagnostics will appear in next run\'s logs.',
    category: 'learning',
  },
  {
    id: 'web-search-enabled',
    date: '2026-02-10',
    title: 'Enable Web Search for Event Discovery',
    problem: 'Web event search was completely disabled — config.searchQueries was an empty array. The only events came from 8 hardcoded scrapers, with no secondary discovery channel. Source discovery used all 5 daily SerpAPI calls searching for listing pages, not individual events.',
    action: 'Connected searchEvents() to the search_queries DB table instead of the empty config array. Split SerpAPI budget: 3 calls for source discovery, 2 for direct event search using least-recently-run queries for rotation.',
    result: 'Agent now has a second discovery channel for finding individual events via web search, using the same managed query pool with priority decay and rotation.',
    category: 'capability',
  },
  {
    id: 'scraper-freshness',
    date: '2026-02-10',
    title: 'Scraper Freshness & Per-Source Observability',
    problem: 'Agent returned the same 54 events every run for 7+ days. Two scrapers (generic.js, websearch.js) had no past-event filtering, continuously returning stale events. All scrapers silently returned empty arrays on failure with no way to detect which sources were broken.',
    action: 'Added past-event filtering to generic.js and websearch.js. Added per-source event count tracking to the pipeline with console warnings when a source returns 0 events. Storing per-source results in agent_runs for persistent observability.',
    result: 'Stale events filtered at the scraper level. Silent scraper failures now visible in logs and stored in the database for Observatory consumption.',
    category: 'optimization',
  },
  {
    id: 'phantom-accepted-fix',
    date: '2026-02-10',
    title: 'Fix Phantom "Accepted" Events in Agent Runs',
    problem: 'Agent reported 2 "accepted" events each run, but no new events appeared on the calendar. Past events fell out of the dedup set (which only fetched future events), so re-scraped past events passed dedup checks, got upserted over themselves, and inflated the eventsAdded counter.',
    action: 'Expanded getExistingEvents() to include events from the last 30 days instead of only future events. This ensures recently-passed events remain in the dedup set and are caught by URL hash matching before reaching upsert.',
    result: 'Eliminated phantom accepted events. The eventsAdded counter now accurately reflects only genuinely new events.',
    category: 'learning',
    commitHash: '754c5f5',
  },
  {
    id: 'location-validation-fix',
    date: '2026-01-30',
    title: 'Critical Location Validation Fix',
    problem: 'Trust tier optimization was skipping ALL validation for trusted sources, including Austin location checks. 26 events were added overnight, most not actually in Austin. Also, AI Accelerator scraper was extracting CSS code instead of event titles.',
    action: 'Added fast string-based Austin location check that runs for ALL events regardless of trust tier. Added malformed title detection for CSS/HTML/code as safety net. Fixed root cause in AI Accelerator scraper: now extracts only direct text nodes and validates before returning.',
    result: 'Location integrity restored while maintaining cost optimization. Non-Austin events now rejected immediately. CSS garbage filtered at both scraper level (root cause) and index level (defense-in-depth).',
    category: 'learning',
    commitHash: '87d337c',
  },
  {
    id: 'observatory-rebuild',
    date: '2026-01-30',
    title: 'Observatory Observability Architecture',
    problem: 'The Observatory page showed agent stats but lacked true observability. No visibility into source health, decision reasoning, costs, or errors. The Evolution Log was a static changelog, not living system telemetry.',
    action: 'Rebuilt Observatory with three-layer architecture: Agent Performance (what it does), Under the Hood (how it thinks/fails), Human Stewardship (how humans guide it). Added Source Health, Decision Log, Cost Tracking, and Error Log components.',
    result: 'True observability: users can see trust tier changes, validation rates, API costs, and errors in real-time. The system tells its own story.',
    category: 'capability',
    commitHash: 'bfd10e9',
  },
  {
    id: 'trust-tiers',
    date: '2026-01-29',
    title: 'Source Trust Tier System',
    problem: 'Every event required Claude validation, even from reliable sources like Meetup groups that consistently post valid AI events. This was costing ~$1.50/day in API calls.',
    action: 'Built a trust tier system: sources start on probation, earn trust through consistent quality (80%+ pass rate), and can be demoted for poor performance.',
    result: 'Trusted sources now skip validation. API costs reduced by ~50%. The agent learns which sources to trust.',
    category: 'optimization',
    commitHash: '570328f',
  },
  {
    id: 'image-location',
    date: '2026-01-14',
    title: 'Image-Based Location Extraction',
    problem: 'Some events had venue information only in their promotional images, not in the text. These events showed "Location: TBD" even when the flyer clearly showed the address.',
    action: 'Added image analysis capability - when an event is missing venue data, the agent now analyzes the event image using Claude\'s vision.',
    result: 'Events with image-only venue info now display correct locations. Filled gaps that text scraping missed.',
    category: 'capability',
    commitHash: '6346226',
  },
  {
    id: 'html-entities',
    date: '2026-01-14',
    title: 'HTML Entity Decoding',
    problem: 'Event titles and descriptions contained raw HTML entities like "&amp;" instead of "&" and "&apos;" instead of apostrophes. Text looked garbled.',
    action: 'Added HTML entity decoding to all scrapers using the he library.',
    result: 'All text now displays cleanly without encoding artifacts.',
    category: 'learning',
    commitHash: '9d85315',
  },
  {
    id: 'sonnet-upgrade',
    date: '2026-01-07',
    title: 'Claude Model Upgrade',
    problem: 'The agent was using an older Claude model. Newer models offer better accuracy and efficiency.',
    action: 'Upgraded to Claude Sonnet 4.5 for all validation, classification, and analysis tasks.',
    result: 'Improved accuracy in event validation and classification while maintaining cost efficiency.',
    category: 'optimization',
    commitHash: 'ac8519d',
  },
  {
    id: 'end-times',
    date: '2026-01-07',
    title: 'End Time Extraction',
    problem: 'Events from web search and Austin AI sources were missing end times. Calendar showed events starting but not when they ended.',
    action: 'Updated websearch.js and austinai.js scrapers to properly extract end times from event data.',
    result: 'Events now display complete time ranges (e.g., "6:00 PM - 8:00 PM" instead of just "6:00 PM").',
    category: 'learning',
    commitHash: '522c283',
  },
  {
    id: 'timezone-fix',
    date: '2026-01-07',
    title: 'Austin Timezone Handling',
    problem: 'Events were showing at wrong times. A 7pm meetup would display as 1am or 1pm due to timezone confusion between UTC and local time.',
    action: 'Audited timezone handling across all scrapers. Applied America/Chicago timezone correctly when sources return local times without timezone info.',
    result: 'All event times now display correctly in Austin local time.',
    category: 'learning',
    commitHash: '4359bec',
  },
  {
    id: 'year-awareness',
    date: '2026-01-05',
    title: 'Dynamic Date Awareness',
    problem: 'Some prompts and scrapers had hardcoded years (e.g., "2025"). As time passed, the agent would reject valid future events or misdate them.',
    action: 'Replaced all hardcoded years with dynamic date generation. The agent now always knows the current date.',
    result: 'Events are correctly dated regardless of when the agent runs. No more year-related bugs.',
    category: 'learning',
    commitHash: 'b0fe36b',
  },
  {
    id: 'single-event-filter',
    date: '2026-01-05',
    title: 'Single Event URL Filtering',
    problem: 'Source discovery was adding individual event URLs (e.g., meetup.com/event/123) instead of event listing pages (e.g., meetup.com/austin-ai/events). This polluted the source list.',
    action: 'Added URL pattern detection to reject single-event URLs and only accept event listing/calendar pages.',
    result: 'Source list stays clean with reusable event listing pages that yield multiple events per scrape.',
    category: 'learning',
    commitHash: '4c69b27',
  },
  {
    id: 'feedback-analysis',
    date: '2026-01-04',
    title: 'Feedback Analysis System',
    problem: 'When users reported events the agent missed, there was no systematic way to learn from this feedback and improve discovery.',
    action: 'Built a feedback analysis system that processes missed event reports, generates new search queries, and identifies potential new sources.',
    result: 'The agent can now learn from user feedback and expand its discovery reach based on what it missed.',
    category: 'capability',
    commitHash: 'be1754b',
  },
  {
    id: 'validation-improvements',
    date: '2026-01-03',
    title: 'Validation Accuracy Improvements',
    problem: 'Event validation had false positives (non-events getting through) and false negatives (valid events being rejected). Organizer and location extraction was inconsistent.',
    action: 'Refined validation prompts, improved organizer extraction logic, and enhanced location parsing.',
    result: 'More accurate event validation with better metadata extraction. Fewer manual corrections needed.',
    category: 'learning',
    commitHash: '67ada3d',
  },
  {
    id: 'run-logging',
    date: '2026-01-03',
    title: 'Agent Run Logging',
    problem: 'No visibility into what the agent was doing. Couldn\'t tell if it ran successfully, how many events it found, or if there were errors.',
    action: 'Created agent_runs table to log every run with detailed stats: events found, sources scraped, API calls made, errors encountered.',
    result: 'Full visibility into agent performance. Enables the Observatory dashboard and debugging.',
    category: 'foundation',
    commitHash: '32e8268',
  },
  {
    id: 'query-prioritization',
    date: '2026-01-03',
    title: 'Smart Query Prioritization',
    problem: 'Search queries were run in arbitrary order. Queries that never found anything were run as often as productive ones. No exploration of new queries.',
    action: 'Implemented priority scoring with time decay. Productive queries get boosted, stale queries deprioritized. Added exploration budget for trying new queries.',
    result: 'More efficient use of limited SerpAPI quota. Better balance between exploitation and exploration.',
    category: 'optimization',
    commitHash: '7d9c0c9',
  },
  {
    id: 'timezone-dedup',
    date: '2025-12-31',
    title: 'Timezone-Aware Deduplication',
    problem: 'The same event posted on different platforms with slight timezone representation differences was being treated as two separate events.',
    action: 'Enhanced duplicate detection to normalize timezones before comparison.',
    result: 'Cross-platform duplicates are now correctly identified and merged.',
    category: 'learning',
    commitHash: '3455aeb',
  },
  {
    id: 'event-summaries',
    date: '2025-12-31',
    title: 'AI-Generated Summaries',
    problem: 'Event descriptions varied wildly in length and quality. Some were walls of text, others too brief. Hard to quickly understand what an event was about.',
    action: 'Added Claude-powered summary generation for each event. Also added detection for paid vs free events.',
    result: 'Every event now has a concise, consistent summary. Users can quickly scan and understand events.',
    category: 'capability',
    commitHash: '2e49b4e',
  },
  {
    id: 'cross-post-dedup',
    date: '2025-12-31',
    title: 'Cross-Platform Duplicate Detection',
    problem: 'Organizers often post the same event on Meetup, Lu.ma, and Eventbrite. The calendar was showing duplicates.',
    action: 'Improved fuzzy matching to detect same event across different platforms by comparing titles, times, and locations.',
    result: 'Duplicate events are merged. Users see each event once regardless of how many platforms list it.',
    category: 'learning',
    commitHash: 'da78d98',
  },
  {
    id: 'autonomous-discovery',
    date: '2025-12-31',
    title: 'Autonomous Source Discovery',
    problem: 'The agent could only scrape sources that were manually added. It couldn\'t find new event sources on its own.',
    action: 'Built web search integration with SerpAPI. The agent now searches for Austin AI events, analyzes results, extracts potential sources, and adds them to its rotation.',
    result: 'The agent continuously discovers new event sources without human intervention. This is the core learning loop.',
    category: 'capability',
    commitHash: '08cfa31',
  },
  {
    id: 'initial-sources',
    date: '2025-12-30',
    title: 'Initial Scraper Suite',
    problem: 'Needed to gather AI events from multiple platforms, each with different page structures and data formats.',
    action: 'Built specialized scrapers for Meetup (GraphQL), Lu.ma (JSON-LD), Eventbrite, Austin Forum, AI Accelerator, Austin AI Alliance, Leaders in AI, and a generic HTML parser.',
    result: '8 working scrapers covering the major Austin AI event sources. Foundation for event discovery.',
    category: 'foundation',
    commitHash: '765ad89',
  },
  {
    id: 'initial-commit',
    date: '2025-12-30',
    title: 'Project Creation',
    problem: 'Austin has a vibrant AI community, but events are scattered across multiple platforms. No single place to find them all.',
    action: 'Created Austin AI Events: a Next.js frontend with Supabase backend, and a Node.js agent that discovers, validates, and curates AI events.',
    result: 'The foundation for an autonomous event discovery system. The agent was born.',
    category: 'foundation',
    commitHash: '3a19903',
  },
];
